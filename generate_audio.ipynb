{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoopGAN — Loop generation with StyleGAN2 and MelGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torchvision import utils\n",
    "from model_drum import Generator\n",
    "import sys\n",
    "sys.path.append('./melgan')\n",
    "from modules import Generator_melgan\n",
    "import os, random\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a pre-trained model in advance\n",
    "\n",
    "``` \n",
    "$ gdown -O drumbeats1_230000.pt 1B3ZWTJFuZbPPH4uIIz-pCcBTLu9-w4nw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants - do not change! \n",
    "N_LATENT = 512\n",
    "N_MLP = 8\n",
    "SIZE_OUTPUT = 64 # size of output image\n",
    "TRUNCATION = 1\n",
    "TRUNCATION_MEAN = 4096\n",
    "SR = 44100\n",
    "\n",
    "# Number of samples (loops) in a batch\n",
    "n_samples = 4  # batch size\n",
    "\n",
    "# name of pre-trained StyleGAN2 model\n",
    "CHECKPOINT = \"./drumbeats1_230000.pt\" \n",
    "\n",
    "# mean / std of Spectrograms of training data. used for the conversion from generated spectrograms into wav files\n",
    "DATAPATH = \"./data/drumbeats_1bar/\" \n",
    "\n",
    "# name of pre-trained MelGAN model\n",
    "MELGAN_MODEL_NAME = \"best_netG.pt\"\n",
    "\n",
    "\n",
    "# Use \"cuda\" if you have GPUs on your machine\n",
    "device_name = \"cuda\"\n",
    "\n",
    "#WSL path\n",
    "WSL_PATH = \"\\\\\\\\wsl.localhost\\\\\\\\Ubuntu-20.04\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a StyleGAN2 model\n",
    "generator = Generator(SIZE_OUTPUT, N_LATENT, N_MLP, channel_multiplier=2).to(device_name)\n",
    "checkpoint = torch.load(CHECKPOINT, map_location=torch.device(device_name))\n",
    "\n",
    "generator.load_state_dict(checkpoint[\"g_ema\"], strict=False)\n",
    "\n",
    "if TRUNCATION < 1:\n",
    "    with torch.no_grad():\n",
    "        mean_latent = generator.mean_latent(TRUNCATION_MEAN)\n",
    "else:\n",
    "    mean_latent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a MelGAN model\n",
    "def load_vocoder(device_name):\n",
    "    feat_dim = 80\n",
    "    mean_fp = f'{DATAPATH}/mean.mel.npy'\n",
    "    std_fp = f'{DATAPATH}/std.mel.npy'\n",
    "    v_mean = torch.from_numpy(np.load(mean_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "    v_std = torch.from_numpy(np.load(std_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "    \n",
    "    vocoder_config_fp = './melgan/args.yml'\n",
    "    vocoder_config = read_yaml(vocoder_config_fp)\n",
    "\n",
    "    n_mel_channels = vocoder_config.n_mel_channels\n",
    "    ngf = vocoder_config.ngf\n",
    "    n_residual_layers = vocoder_config.n_residual_layers\n",
    "\n",
    "    vocoder = Generator_melgan(n_mel_channels, ngf, n_residual_layers).to(device_name)\n",
    "    vocoder.eval()\n",
    "\n",
    "    vocoder_param_fp = os.path.join('./melgan', MELGAN_MODEL_NAME)\n",
    "    vocoder.load_state_dict(torch.load(vocoder_param_fp, map_location=torch.device(device_name)), strict=False)\n",
    "\n",
    "    return vocoder, v_mean, v_std\n",
    "\n",
    "VOCODER, V_MEAN, V_STD = load_vocoder(device_name)\n",
    "\n",
    "def vocode(sample, vocoder=VOCODER, v_mean=V_MEAN, v_std=V_STD):\n",
    "    de_norm = sample.squeeze(0) * v_std + v_mean\n",
    "    audio_output = vocoder(de_norm)\n",
    "    return audio_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANSpace \n",
    "Härkönen, Erik, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. “GANSpace: Discovering Interpretable GAN Controls.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/2004.02546."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0651, -0.0861, -0.0172,  ..., -0.0100,  0.0046,  0.0157],\n",
      "        [-0.0640,  0.0295, -0.0153,  ...,  0.0014, -0.0239,  0.0266]])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate random W (style vectors) with random Z \n",
    "randz = torch.randn(2000, N_LATENT, device=device_name) \n",
    "randw = generator.get_latent(randz).detach().cpu().numpy()\n",
    "\n",
    "# Find principal components using PCA \n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(randw)\n",
    "pcomponents = torch.tensor(pca.components_)\n",
    "print(pcomponents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n",
    "\n",
    "To make it easier to handle on Max/MSP, generated loops is saved as a wav file with `n_samples` channels. (default: 4ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "prev_w = None\n",
    "\n",
    "# Interpolation of z \n",
    "def get_w(w, x, y, scale=1.0):\n",
    "    w_diff = (pcomponents * torch.tensor([[x, y]]).T).to(device_name)\n",
    "    for i in range(w_diff.shape[0]):\n",
    "        w[i] = w_diff[0] + w_diff[1] + w[i]\n",
    "    return w\n",
    "\n",
    "# main function\n",
    "# g_ema: stylegan generator\n",
    "# center_z: to specify the input latent z \n",
    "# trucation: GAN trucation value\n",
    "# variation: the scale of noise added to center_z (= vatiation in a batch)\n",
    "def generate(g_ema=generator, center_z=None, use_prev_w = False, truncation=TRUNCATION, variation=0.10, w1=0.0, w2=0.0):\n",
    "    global prev_w\n",
    "\n",
    "    print(\"generating...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        g_ema.eval()\n",
    "        \n",
    "        if not use_prev_w or prev_w == None:\n",
    "            if center_z is None:\n",
    "                # random init\n",
    "                sample_z = torch.randn(1, N_LATENT, device=device_name)\n",
    "                sample_z = sample_z.repeat(n_samples, 1) +  torch.randn(n_samples, N_LATENT, device=device_name) * variation\n",
    "            else:\n",
    "                # use specific latent z\n",
    "                sample_z = center_z + torch.randn(n_samples, N_LATENT, device=device_name) * variation\n",
    "            sample_z = sample_z.float()\n",
    "            \n",
    "            # Generate!!!            \n",
    "            sample_w = g_ema.get_latent(sample_z)\n",
    "        else:\n",
    "            sample_w = prev_w\n",
    "        sample_w = get_w(sample_w, w1, w1)\n",
    "\n",
    "        # store for the next round\n",
    "        prev_w = sample_w\n",
    "\n",
    "        sample_w = sample_w.unsqueeze(1).repeat(1, N_LATENT, 1)    \n",
    "        sample, _ = g_ema([sample_w], truncation=truncation, truncation_latent=mean_latent, input_is_latent=True)\n",
    "\n",
    "        # Saving tje generated spectrogram image\n",
    "        randid = random.randint(0, 10000)\n",
    "        imagepath = f'/tmp/img_{randid}.png'\n",
    "        utils.save_image(sample, \"/mnt/c\" + imagepath, nrow=1, normalize=True, range=(-1, 1))\n",
    "    \n",
    "        # Saving multi channel audio file\n",
    "        filepath = f'/tmp/gem_{randid}.wav'\n",
    "        channels = [] \n",
    "        numpy_chs = []  # for numpy array\n",
    "        # convert (n_samples) spectrogram into audio, one by one \n",
    "        for i in range(n_samples):\n",
    "            audio_output = vocode(sample[i])\n",
    "            audio_output = audio_output.squeeze().detach().cpu().numpy() \n",
    "            numpy_chs.append(audio_output)\n",
    "            \n",
    "            channel = AudioSegment( (audio_output*np.iinfo(np.int16).max).astype(\"int16\").tobytes(), sample_width=2, # 16 bit \n",
    "                    frame_rate=SR, channels=1)\n",
    "            channels.append(channel)\n",
    "        # save as a wav file with (n_samples) channel\n",
    "        multich = AudioSegment.from_mono_audiosegments(*channels)\n",
    "        multich.export(\"/mnt/c\"+filepath, format=\"wav\")\n",
    "\n",
    "        return filepath, imagepath, np.array(numpy_chs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Let's generate random loops with random input latent z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/c/mnt/tmp/img_9476.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# random generation \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m _,  imagepath, audio_output \u001b[39m=\u001b[39m generate(variation\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mshape:\u001b[39m\u001b[39m\"\u001b[39m, audio_output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[39m# display the generated spectrogram\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(g_ema, center_z, use_prev_w, truncation, variation, w1, w2)\u001b[0m\n\u001b[1;32m     48\u001b[0m randid \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m10000\u001b[39m)\n\u001b[1;32m     49\u001b[0m imagepath \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/tmp/img_\u001b[39m\u001b[39m{\u001b[39;00mrandid\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 50\u001b[0m utils\u001b[39m.\u001b[39;49msave_image(sample, \u001b[39m\"\u001b[39;49m\u001b[39m/c/mnt\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m imagepath, nrow\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39mrange\u001b[39;49m\u001b[39m=\u001b[39;49m(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m     52\u001b[0m \u001b[39m# Saving multi channel audio file\u001b[39;00m\n\u001b[1;32m     53\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/tmp/gem_\u001b[39m\u001b[39m{\u001b[39;00mrandid\u001b[39m}\u001b[39;00m\u001b[39m.wav\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/aidj3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/aidj3/lib/python3.8/site-packages/torchvision/utils.py:151\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39madd_(\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    150\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 151\u001b[0m im\u001b[39m.\u001b[39;49msave(fp, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aidj3/lib/python3.8/site-packages/PIL/Image.py:2429\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2427\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2428\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2429\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mw+b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2431\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2432\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/c/mnt/tmp/img_9476.png'"
     ]
    }
   ],
   "source": [
    "# random generation \n",
    "_,  imagepath, audio_output = generate(variation=0.0)\n",
    "\n",
    "print(\"shape:\", audio_output.shape)\n",
    "\n",
    "# display the generated spectrogram\n",
    "ipd.display(ipd.Image(filename=imagepath))\n",
    "\n",
    "\n",
    "# play the first channel\n",
    "ipd.display(ipd.Audio(audio_output[0], rate=SR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stereo effect\n",
    "Small gausian noise can be added to the input latent vector z. If you play the first and second sample in the generated batch as a stereo audio file, then you'll get an interesting stereo effect. Try different numbers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,  imagepath, audio_output = generate(variation=0.35)\n",
    "\n",
    "ipd.display(ipd.Audio(audio_output[:2], rate=SR)) # stereo \n",
    "\n",
    "ipd.display(ipd.Image(filename=imagepath))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSC server/client for Max/MSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving on ('172.17.140.208', 10015)\n",
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('172.17.128.1', 58322)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/socketserver.py\", line 683, in process_request_thread\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/pythonosc/osc_server.py\", line 33, in handle\n",
      "    server.dispatcher.call_handlers_for_packet(self.request[0], self.client_address)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/pythonosc/dispatcher.py\", line 193, in call_handlers_for_packet\n",
      "    handler.invoke(client_address, timed_msg.message)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/pythonosc/dispatcher.py\", line 56, in invoke\n",
      "    self.callback(message.address, *message)\n",
      "  File \"/tmp/ipykernel_4898/1813724069.py\", line 10, in generate_random\n",
      "    audiopath, imagepath, _ = generate(center_z=None, use_prev_w=False, variation=variation, w1=x, w2=y) # random sample\n",
      "  File \"/tmp/ipykernel_4898/3827772622.py\", line 50, in generate\n",
      "    utils.save_image(sample, \"/c/mnt\" + imagepath, nrow=1, normalize=True, range=(-1, 1))\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/torchvision/utils.py\", line 151, in save_image\n",
      "    im.save(fp, format=format)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/PIL/Image.py\", line 2429, in save\n",
      "    fp = builtins.open(filename, \"w+b\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/c/mnt/tmp/img_6756.png'\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('172.17.128.1', 58322)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/socketserver.py\", line 683, in process_request_thread\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/pythonosc/osc_server.py\", line 33, in handle\n",
      "    server.dispatcher.call_handlers_for_packet(self.request[0], self.client_address)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/pythonosc/dispatcher.py\", line 193, in call_handlers_for_packet\n",
      "    handler.invoke(client_address, timed_msg.message)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/pythonosc/dispatcher.py\", line 56, in invoke\n",
      "    self.callback(message.address, *message)\n",
      "  File \"/tmp/ipykernel_4898/1813724069.py\", line 10, in generate_random\n",
      "    audiopath, imagepath, _ = generate(center_z=None, use_prev_w=False, variation=variation, w1=x, w2=y) # random sample\n",
      "  File \"/tmp/ipykernel_4898/3827772622.py\", line 50, in generate\n",
      "    utils.save_image(sample, \"/c/mnt\" + imagepath, nrow=1, normalize=True, range=(-1, 1))\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/torchvision/utils.py\", line 151, in save_image\n",
      "    im.save(fp, format=format)\n",
      "  File \"/home/nao/anaconda3/envs/aidj3/lib/python3.8/site-packages/PIL/Image.py\", line 2429, in save\n",
      "    fp = builtins.open(filename, \"w+b\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/c/mnt/tmp/img_751.png'\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server, udp_client\n",
    "import os, random\n",
    "\n",
    "# client\n",
    "client = udp_client.SimpleUDPClient('192.168.30.15', 10018)\n",
    "\n",
    "# generate randomly\n",
    "def generate_random(unused_addr, x, y, variation=0.1):\n",
    "    audiopath, imagepath, _ = generate(center_z=None, use_prev_w=False, variation=variation, w1=x, w2=y) # random sample\n",
    "    client.send_message(\"/generated\", (audiopath, imagepath)) # init both R and L\n",
    "    \n",
    "# morphing\n",
    "def generate_xy(unused_addr, x, y, variation=0.0):\n",
    "    audiopath, imagepath, _  = generate(center_z=None, use_prev_w=True, variation=variation, w1=x, w2=y) # random sample\n",
    "    client.send_message(\"/generated\", (audiopath, imagepath))\n",
    "\n",
    "# server\n",
    "dispatcher = dispatcher.Dispatcher()\n",
    "dispatcher.map(\"/generate_random\", generate_random)\n",
    "dispatcher.map(\"/generate_xy\", generate_xy)\n",
    "\n",
    "server = osc_server.ThreadingOSCUDPServer(\n",
    "    ('172.17.140.208', 10015), dispatcher)\n",
    "print(\"Serving on {}\".format(server.server_address))\n",
    "server.serve_forever()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5cca8b1728c569639938642ad291f1ea926bc04e4007ecfce4c4d99545e9079"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
